{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Search Engine.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MARwMybKIIKu",
        "colab_type": "text"
      },
      "source": [
        "To install libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk69gSInIAdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install b4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQER2T81JC6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiYVimPPJbPU",
        "colab_type": "text"
      },
      "source": [
        "Code "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xz0lGBDDIAdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests \n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "query = input(\"Enter word to search: \")\n",
        "url = f\"https://google.com/search?q={query}\" \n",
        "res = requests.get(url, headers={\"User-Agent\":\"Mozilla/0.5\"})\n",
        "\n",
        "soup = BeautifulSoup(res.content, \"html.parser\")\n",
        "result_div = soup.find_all('div', attrs = {'class': 'ZINbbc'})\n",
        "\n",
        "links = []\n",
        "titles = []\n",
        "descriptions = []\n",
        "\n",
        "for r in result_div:\n",
        "    try:\n",
        "        link = r.find('a', href = True)\n",
        "        title = r.find('div', attrs={'class':'vvjwJb'}).get_text()\n",
        "        description = r.find('div', attrs={'class':'s3v9rd'}).get_text()\n",
        "\n",
        "        if link != '' and title != '' and description != '': \n",
        "            links.append(link['href'])\n",
        "            titles.append(title)\n",
        "            descriptions.append(description)\n",
        "    except:\n",
        "        continue\n",
        "        \n",
        "to_remove = []\n",
        "clean_links = []\n",
        "for i, l in enumerate(links):\n",
        "    clean = re.search('\\/url\\?q\\=(.*)\\&sa',l)\n",
        "    \n",
        "    if clean is None:\n",
        "        to_remove.append(i)\n",
        "        continue\n",
        "    clean_links.append(clean.group(1))\n",
        "\n",
        "for x in to_remove:\n",
        "    del titles[x]\n",
        "    del descriptions[x]\n",
        "    \n",
        "    \n",
        "List_all_rank = []\n",
        "def Sort_Tuple(tup,ind):\n",
        "    tup.sort(key = lambda x: x[ind])  \n",
        "    return tup  \n",
        "\n",
        "def lesk(query, sentence,ind):\n",
        "    Text1 = sentence.lower()\n",
        "    words = nltk.word_tokenize(Text1)\n",
        "    \n",
        "    stop_words = stopwords.words(\"english\")\n",
        "    stop_words += ['can', 'will', 'use', 'one', 'using', 'used', 'also', 'see', 'first', 'like']\n",
        "    stop_words += ['page', 'get', 'new', 'two', 'site', 'blog', 'many', 'may' ,\"don't\", 'dont', 'way']\n",
        "    stop_words += ['last', 'best', 'able', 'even', 'next', 'last', 'let', \"none\", 'every', 'three']\n",
        "    stop_words += ['lot', 'well', 'chart', 'much', 'based', 'important', 'posts', 'reads', 'least']\n",
        "    stop_words += ['still', 'follow', 'called', 'and','this', 'that', 'there', 'as','the', 'is']\n",
        "    stop_words += ['/', '=', '.', ',', '.']\n",
        "\n",
        "    filtered_words = []\n",
        "    for word in words:\n",
        "        if word not in stop_words:\n",
        "            filtered_words.append(word)\n",
        "    word = query\n",
        "    len_syn = len(wordnet.synsets(word))\n",
        "    \n",
        "    da=[]\n",
        "    ea=[]\n",
        "    List_rank = []\n",
        "    for x in range(len_syn):\n",
        "\n",
        "        for i in filtered_words:\n",
        "            synset = wordnet.synsets(word)[x]\n",
        "            if i in synset.definition().lower():\n",
        "                if i in da:\n",
        "                    pass\n",
        "                else:\n",
        "                    da.append(i)\n",
        "              \n",
        "            if len(synset.examples()) == 0:\n",
        "                pass\n",
        "            else:\n",
        "                if i in synset.examples()[0].lower() and word in synset.examples()[0].lower():\n",
        "                    if i in ea:\n",
        "                        pass\n",
        "                    else:\n",
        "                        ea.append(i)\n",
        "                        \n",
        "        l = []\n",
        "        l.insert(0,x)\n",
        "        r = len(da)+len(ea)\n",
        "        l.append(r)\n",
        "        y2 = tuple(l)\n",
        "        List_rank.append(y2)\n",
        "    Sorted_list_rank = Sort_Tuple(List_rank,1)\n",
        "    l = list(Sorted_list_rank[-1])\n",
        "    l.insert(0,ind)\n",
        "    y2 = tuple(l)                           \n",
        "    List_all_rank.append(y2)\n",
        "    \n",
        "for idx, val in enumerate(descriptions):\n",
        "    lesk(query, val, idx)\n",
        "\n",
        "s_li = Sort_Tuple(List_all_rank,2)\n",
        "for x in range(len(List_all_rank),0,-1):\n",
        "    print(\"Page Number\",s_li[x-1][0]+1, \", Most Synset Num\",s_li[x-1][1],\", With Rank\",s_li[x-1][2])\n",
        "    print(\"Title:\",titles[s_li[x-1][0]])\n",
        "    print(\"Link:\",clean_links[s_li[x-1][0]])\n",
        "    print(\"-------------------------\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0apka-GEIAdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJvgVkAhIAdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRHqYVtBI_pd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}